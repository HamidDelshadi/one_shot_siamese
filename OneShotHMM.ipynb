{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "OneShotHMM.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "nHBIe8GsCJFI",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592953162690,
     "user_tz": -120,
     "elapsed": 2370,
     "user": {
      "displayName": "Hamid Delshadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi909tRlErfOZIjLhsIJyFkWC5Z1aNRB90Ugy2q=s64",
      "userId": "06582474430818390941"
     }
    },
    "outputId": "7cfba3ea-db3d-49b1-d604-6322a57874c6"
   },
   "source": [
    "import os\n",
    "import keras\n",
    "import logging\n",
    "import numpy as np\n",
    "from imageio import imread\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image, ImageFilter\n",
    "from keras.models import Sequential\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITb_buRIDkjl",
    "colab_type": "text"
   },
   "source": [
    "### Dataset managment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EEQcakdrgPMr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def download_ds(download_address, save_address):\n",
    "    if os.path.exists(save_address):\n",
    "        logger.info(\"The dataset has already been downloaded.\")\n",
    "    else:\n",
    "        !wget -O ds.zip {download_address}\n",
    "        !unzip ds.zip\n",
    "        !mv ShapesData_few-show-learning {save_address}\n",
    "        !find {save_address} -name '*.DS_Store' -delete"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "segFuH7zs7Pl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class DS:\n",
    "    def __init__(self):\n",
    "        self.x_tr = None\n",
    "        self.y_tr = None\n",
    "        self.labels_tr = None\n",
    "        self.x_val = None\n",
    "        self.Y_val = None\n",
    "        self.labels_val = None\n",
    "        self.X_eval = None\n",
    "        self.Y_eval = None\n",
    "        self.labels_eval = None\n",
    "        self.labels_dict = None\n",
    "\n",
    "    def __Load_data(self,address, new_im_size, train_ratio):\n",
    "        class_count = len(os.listdir(address))\n",
    "        train_count = int(class_count*train_ratio)\n",
    "        train_idx = np.random.choice(class_count, size = train_count, replace = False)\n",
    "        x_tr, y_tr, x_val, y_val = [],[],[],[]\n",
    "        labels_idx = dict()\n",
    "        labels_tr = []\n",
    "        labels_val = []\n",
    "        offset_tr, offset_val = 0,0\n",
    "        if os.path.exists(os.path.join(address,\".DS_Store\")):\n",
    "            os.remove(os.path.join(address, \".DS_Store\"))\n",
    "        for i , label in enumerate(os.listdir(address)):\n",
    "            labels_address = os.path.join(address, label)\n",
    "            x, y, labels = (x_tr, y_tr, labels_tr) if i in train_idx else (x_val, y_val, labels_val)\n",
    "            labels_idx[i] = label\n",
    "            for image_name in os.listdir(labels_address):\n",
    "                image_address = os.path.join(labels_address, image_name)\n",
    "                try:\n",
    "                    img = Image.open(image_address).convert('RGB')\n",
    "                    img = img.resize((new_im_size, new_im_size))\n",
    "                    x.append(np.array(img, dtype=np.float32))\n",
    "                    y.append(i)\n",
    "                except:\n",
    "                    logger.info(\"ERROR: can't load {}\".format(image_address))\n",
    "            label_count = len(os.listdir(labels_address))\n",
    "            if i in train_idx:\n",
    "                labels.append((i, label_count , offset_tr))\n",
    "                offset_tr+=label_count\n",
    "            else:\n",
    "                labels.append((i, label_count , offset_val))\n",
    "                offset_val+=label_count\n",
    "\n",
    "        labels = labels_idx\n",
    "        x_tr = np.array(x_tr)\n",
    "        y_tr = np.array(y_tr)\n",
    "                \n",
    "        x_val = np.array(x_val)\n",
    "        y_val = np.array(y_val)\n",
    "\n",
    "        if class_count-train_count > 0:\n",
    "            x_val = np.array(x_val)\n",
    "            y_val = np.array(y_val)\n",
    "\n",
    "        return x_tr, y_tr, np.array(labels_tr), x_val, y_val, np.array(labels_val), labels\n",
    "\n",
    "    def Load_validaton(self, address, new_im_size):\n",
    "        self.x_eval, self.eval, self.labels_eval, _, _, _, _ =self.__Load_data(address, new_im_size, 1)\n",
    "    def Load_background(self,address, new_im_size, train_ratio):\n",
    "        self.x_tr, self.y_tr, self.labels_tr, self.x_val, self.y_val, self.labels_val, self.labels=self.__Load_data(address, new_im_size, train_ratio)\n",
    "\n",
    "    def get_paired_data(self, batch_size, mode='train'):\n",
    "        if mode=='train':\n",
    "            x, labels = self.x_tr, self.labels_tr\n",
    "        elif mode=='validation':\n",
    "            x, labels = self.x_val, self.labels_val\n",
    "        else:\n",
    "            x, labels = self.x_eval, self.labels_eval\n",
    "\n",
    "        half_size = batch_size // 2\n",
    "        class_count = labels.shape[0]\n",
    "        rnd_classes = np.random.choice(class_count, size = half_size, replace = True)\n",
    "        rnd_pos_classes_lbl =labels[rnd_classes]\n",
    "        rnd_1 = np.rint(np.random.uniform(0,1, half_size)*rnd_pos_classes_lbl[:,1]+rnd_pos_classes_lbl[:,2]).astype(np.int32)\n",
    "        rnd_pos_2 = np.rint(np.random.uniform(0,1, half_size)*rnd_pos_classes_lbl[:,1] +rnd_pos_classes_lbl[:,2]).astype(np.int32)\n",
    "\n",
    "        rnd_neg_2_classes =  (rnd_classes + np.random.randint(1,class_count-1,half_size)) % class_count\n",
    "        rnd_neg_classes_lbl =labels[rnd_neg_2_classes]\n",
    "        rnd_neg_2 = np.rint(np.random.uniform(0,1, half_size)*rnd_neg_classes_lbl[:,1]+rnd_neg_classes_lbl[:,2]).astype(np.int32)\n",
    "        \n",
    "        pair_1 = x[np.repeat(rnd_1,2)]\n",
    "        pair_2 = np.concatenate((x[rnd_pos_2], x[rnd_neg_2]))\n",
    "        y = np.ones(half_size*2)\n",
    "        y[half_size:]= 0\n",
    "        return [pair_1, pair_2], y\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s2ijXIHLR_rd",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "download_address = \"https://www.dropbox.com/s/9imgl9m0qzin55g/ShapesData_few-show-learning.zip?dl=0\"\n",
    "save_address = \"dataset\"\n",
    "new_im_size = 105\n",
    "download_ds(download_address, save_address)\n",
    "batch1_address = os.path.join(save_address, \"batch1\")\n",
    "batch1_ds = DS()\n",
    "batch1_ds.Load_background(batch1_address, 105, 0.9)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PQOmwFS4Ijdo",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592953168907,
     "user_tz": -120,
     "elapsed": 8530,
     "user": {
      "displayName": "Hamid Delshadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi909tRlErfOZIjLhsIJyFkWC5Z1aNRB90Ugy2q=s64",
      "userId": "06582474430818390941"
     }
    },
    "outputId": "c8510e96-2ace-45ef-ab71-f189fee31889"
   },
   "source": [
    "print(batch1_ds.labels)\n",
    "print(batch1_ds.x_tr.shape)\n",
    "print(batch1_ds.y_tr.shape)\n",
    "print(batch1_ds.labels_tr)\n",
    "print(batch1_ds.x_val.shape)\n",
    "print(batch1_ds.y_val.shape)\n",
    "print(batch1_ds.labels_val)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "{0: 'Boy', 1: 'Monkey', 2: 'Bat', 3: 'If', 4: 'Tree', 5: 'House', 6: 'Lion', 7: 'Crocodile', 8: 'Girl', 9: 'Car'}\n",
      "(5775, 105, 105, 3)\n",
      "(5775,)\n",
      "[[   0  691    0]\n",
      " [   1  287  691]\n",
      " [   2  391  978]\n",
      " [   3  468 1369]\n",
      " [   4  859 1837]\n",
      " [   5  737 2696]\n",
      " [   6 1144 3433]\n",
      " [   7  649 4577]\n",
      " [   9  549 5226]]\n",
      "(1315, 105, 105, 3)\n",
      "(1315,)\n",
      "[[   8 1315    0]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zjw860CkObXK",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592953168908,
     "user_tz": -120,
     "elapsed": 8517,
     "user": {
      "displayName": "Hamid Delshadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi909tRlErfOZIjLhsIJyFkWC5Z1aNRB90Ugy2q=s64",
      "userId": "06582474430818390941"
     }
    },
    "outputId": "385fb571-8ea2-4e42-8cf6-65c18547b57b"
   },
   "source": [
    "batch, y = batch1_ds.get_paired_data(100, 'train')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[4 6 6 5 7 3 7 4 8 6 0 5 5 5 3 0 5 4 7 0 4 3 5 5 2 1 1 6 0 1 3 1 2 1 1 8 8\n",
      " 5 4 8 1 0 3 6 3 6 0 2 5 6]\n",
      "[0 8 8 0 3 7 0 2 1 2 3 0 8 8 5 2 6 8 0 7 8 1 7 2 6 4 4 0 6 6 0 8 0 7 4 4 3\n",
      " 8 6 4 3 4 1 3 5 7 6 5 8 8]\n",
      "[-4  2  2 -5 -4  4 -7 -2 -7 -4  3 -5  3  3  2  2  1  4 -7  7  4 -2  2 -3\n",
      "  4  3  3 -6  6  5 -3  7 -2  6  3 -4 -5  3  2 -4  2  4 -2 -3  2  1  6  3\n",
      "  3  2]\n",
      "(100, 105, 105, 3)\n",
      "(100, 105, 105, 3)\n",
      "(100,)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPxZDNUlZ5JY",
    "colab_type": "text"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6daJpLGFZ7UM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def siamese(input_shape):\n",
    "    init_b = keras.initializers.RandomNormal(mean = 0.5, stddev=0.01)\n",
    "    init_w = keras.initializers.RandomNormal(mean = 0.0, stddev=0.01)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,kernel_initializer=init_w, bias_initializer=init_b, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu', kernel_initializer=init_w, bias_initializer=init_b, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=init_w, bias_initializer=init_b, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=init_w, bias_initializer=init_b, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',kernel_regularizer=l2(1e-3), kernel_initializer=init_w,bias_initializer=init_b))\n",
    "    dense_layer = Dense(1,activation='sigmoid',bias_initializer=init_b)\n",
    "    L1_layer = Lambda(lambda tensors:keras.backend.abs(tensors[0] - tensors[1]))\n",
    "    input_1, input_2 = Input(input_shape), Input(input_shape)\n",
    "    outputs = dense_layer(L1_layer([model(input_1), model(input_2)]))\n",
    "    siamese_model = Model(inputs=[input_1,input_2],outputs=outputs)\n",
    "    return siamese_model\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G-0qu6hsaKdH",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592953169652,
     "user_tz": -120,
     "elapsed": 9237,
     "user": {
      "displayName": "Hamid Delshadi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi909tRlErfOZIjLhsIJyFkWC5Z1aNRB90Ugy2q=s64",
      "userId": "06582474430818390941"
     }
    },
    "outputId": "3b5ee033-05ae-4532-c355-44b504bd0b1d"
   },
   "source": [
    "model = siamese((new_im_size, new_im_size, 1))\n",
    "model.summary()\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4096)         38947648    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,951,745\n",
      "Trainable params: 38,951,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Training\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(max_epoch, ds):\n",
    "    Logger.debug(\"training started\")\n",
    "    for i in range(1, max_epoch+1):\n",
    "        (inputs,targets) = get_batch(batch_size)\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "        if i % evaluate_every == 0:\n",
    "            print(\"\\n ------------- \\n\")\n",
    "            print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "            print(\"Train Loss: {0}\".format(loss)) \n",
    "            val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "            model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "            if val_acc >= best:\n",
    "                print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "                best = val_acc\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJ6uUaupqO0O",
    "colab_type": "text"
   },
   "source": [
    "def train(max_epoch, ds):\n",
    "    Logger.debug(\"training started\")\n",
    "    for i in range(1, max_epoch+1):\n",
    "        (inputs,targets) = get_batch(batch_size)\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "        if i % evaluate_every == 0:\n",
    "            print(\"\\n ------------- \\n\")\n",
    "            print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "            print(\"Train Loss: {0}\".format(loss)) \n",
    "            val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "            model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "            if val_acc >= best:\n",
    "                print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "                best = val_acc\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b7d1-J4jaU02",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train(max_epoch, ds):\n",
    "    Logger.debug(\"training started\")\n",
    "    for i in range(1, max_epoch+1):\n",
    "        (inputs,targets) = get_batch(batch_size)\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "        if i % evaluate_every == 0:\n",
    "            print(\"\\n ------------- \\n\")\n",
    "            print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "            print(\"Train Loss: {0}\".format(loss)) \n",
    "            val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "            model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "            if val_acc >= best:\n",
    "                print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "                best = val_acc\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}